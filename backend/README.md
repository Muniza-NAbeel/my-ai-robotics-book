---
title: AI Robotics Book Backend
emoji: ðŸ¤–
colorFrom: blue
colorTo: purple
sdk: docker
pinned: false
---

# RAG Chatbot Backend

This is a production-ready FastAPI backend for a Retrieval-Augmented Generation (RAG) chatbot that processes Docusaurus book content.

## Features

- **Markdown Ingestion**: Upload and process markdown files
- **Text Chunking**: Split documents into manageable chunks with overlap
- **Embedding Generation**: Create embeddings using Cohere
- **Vector Storage**: Store embeddings in Qdrant vector database
- **Semantic Search**: Query vectors using semantic similarity
- **OpenAI Integration**: Answer questions using OpenAI Agents
- **Dual Answer Modes**: Support for normal and selected-text-only answering
- **Comprehensive Logging**: Full logging for requests, errors, and embeddings

## Architecture

```
backend/
â”œâ”€â”€ rag_chatbot_backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py              # FastAPI app entrypoint
â”‚   â”‚   â”œâ”€â”€ config.py            # Environment variable loading
â”‚   â”‚   â”œâ”€â”€ embeddings.py        # Cohere embeddings functions
â”‚   â”‚   â”œâ”€â”€ qdrant_client.py     # Qdrant operations
â”‚   â”‚   â”œâ”€â”€ agent.py             # RAG agent with OpenAI
â”‚   â”‚   â”œâ”€â”€ tools/
â”‚   â”‚   â”‚   â””â”€â”€ retrieve.py      # Retrieval tool
â”‚   â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”‚   â””â”€â”€ chat.py          # API endpoints
â”‚   â”‚   â””â”€â”€ ingestion/
â”‚   â”‚       â”œâ”€â”€ ingest.py        # Document ingestion
â”‚   â”‚       â””â”€â”€ chunking.py      # Text chunking
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ .env.example                 # Environment variables template
â””â”€â”€ README.md                    # This file
```

## Setup

1. Clone the repository and navigate to the backend directory
2. Create a virtual environment:
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```
3. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```
4. Copy the environment template and fill in your API keys:
   ```bash
   cp .env.example .env
   # Edit .env with your actual API keys
   ```
5. Run the application:
   ```bash
   cd rag_chatbot_backend/app
   uvicorn main:app --reload --host 0.0.0.0 --port 8000
   ```

## API Endpoints

- `POST /api/v1/ingest/upload` - Upload and process markdown documents
- `POST /api/v1/chat/session` - Create a new chat session
- `POST /api/v1/chat/message` - Send a message and get a response
- `GET /api/v1/health` - Health check
- `GET /api/v1/stats` - Get vector store statistics

## Environment Variables

- `COHERE_API_KEY`: Your Cohere API key
- `QDRANT_API_KEY`: Your Qdrant API key
- `QDRANT_HOST`: Your Qdrant Cloud endpoint
- `OPENAI_API_KEY`: Your OpenAI API key
- `DEBUG`: Enable debug mode (default: False)
- `LOG_LEVEL`: Logging level (default: INFO)
- `QDRANT_COLLECTION_NAME`: Name of the Qdrant collection (default: documents)
- `CHUNK_SIZE`: Size of text chunks (default: 1200)
- `CHUNK_OVERLAP`: Overlap between chunks (default: 100)

## Usage

1. Upload a markdown document using the `/api/v1/ingest/upload` endpoint
2. Create a chat session using `/api/v1/chat/session`
3. Send questions using `/api/v1/chat/message` with the session ID

## Security

- All sensitive configuration is loaded from environment variables
- API keys are never hardcoded in the source code
- Input validation is performed on all endpoints
- CORS is configured (though set to allow all origins in development)